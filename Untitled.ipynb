{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da419049-9527-48bf-90d4-05d31b34f629",
   "metadata": {},
   "source": [
    "# TF-IDF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fa7469-2a2b-468e-b6d3-777741807b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61737fcd-c48c-4deb-abcb-a8d9e1c33381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>final_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1387917769501515778</td>\n",
       "      <td>2021-04-29 23:52:37+00:00</td>\n",
       "      <td>MaseebAkhter</td>\n",
       "      <td>24.972077</td>\n",
       "      <td>67.064381</td>\n",
       "      <td>rd jumma mubarak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1387914934533513220</td>\n",
       "      <td>2021-04-29 23:41:21+00:00</td>\n",
       "      <td>PolsekPlaju3</td>\n",
       "      <td>-3.190822</td>\n",
       "      <td>104.794609</td>\n",
       "      <td>lawan covid sinergitas tni polri cegah tular c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1387913431865610244</td>\n",
       "      <td>2021-04-29 23:35:22+00:00</td>\n",
       "      <td>irfania_real</td>\n",
       "      <td>31.315389</td>\n",
       "      <td>74.222395</td>\n",
       "      <td>jummah mubarak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1387911958393987075</td>\n",
       "      <td>2021-04-29 23:29:31+00:00</td>\n",
       "      <td>kenny_thok</td>\n",
       "      <td>-2.455096</td>\n",
       "      <td>111.930698</td>\n",
       "      <td>semangat ya neng isolasi pakai sek pondok deri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1387907774441566213</td>\n",
       "      <td>2021-04-29 23:12:54+00:00</td>\n",
       "      <td>kejarirohul</td>\n",
       "      <td>0.902542</td>\n",
       "      <td>100.308263</td>\n",
       "      <td>kamis tanggal april kepala seksi intelijen ari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8109</th>\n",
       "      <td>9332</td>\n",
       "      <td>1536673294782709760</td>\n",
       "      <td>2022-06-14 11:33:58+00:00</td>\n",
       "      <td>StellaSuwardi</td>\n",
       "      <td>-6.297602</td>\n",
       "      <td>106.770437</td>\n",
       "      <td>vaksinasi booster tingkat imunitas varian omic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8110</th>\n",
       "      <td>9333</td>\n",
       "      <td>1536525461077262336</td>\n",
       "      <td>2022-06-14 01:46:31+00:00</td>\n",
       "      <td>natasya_puspa4</td>\n",
       "      <td>-6.364100</td>\n",
       "      <td>106.799599</td>\n",
       "      <td>jokowi menteri waspada hati hati varian omicro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8111</th>\n",
       "      <td>9335</td>\n",
       "      <td>1536515216665899013</td>\n",
       "      <td>2022-06-14 01:05:49+00:00</td>\n",
       "      <td>Agistasyahka</td>\n",
       "      <td>-6.301652</td>\n",
       "      <td>106.974561</td>\n",
       "      <td>sebar virus omicron ba ba cepat banding varian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>9336</td>\n",
       "      <td>1536358141377384448</td>\n",
       "      <td>2022-06-13 14:41:39+00:00</td>\n",
       "      <td>Ardi_Wdyto</td>\n",
       "      <td>-7.776581</td>\n",
       "      <td>113.198255</td>\n",
       "      <td>varian omicron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>9337</td>\n",
       "      <td>1536251604772745216</td>\n",
       "      <td>2022-06-13 07:38:19+00:00</td>\n",
       "      <td>gemaposID</td>\n",
       "      <td>-6.218042</td>\n",
       "      <td>106.857762</td>\n",
       "      <td>breaking news omicron ba ba resmi deteksi indo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8114 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                   id                       date  \\\n",
       "0              0  1387917769501515778  2021-04-29 23:52:37+00:00   \n",
       "1              2  1387914934533513220  2021-04-29 23:41:21+00:00   \n",
       "2              3  1387913431865610244  2021-04-29 23:35:22+00:00   \n",
       "3              4  1387911958393987075  2021-04-29 23:29:31+00:00   \n",
       "4              5  1387907774441566213  2021-04-29 23:12:54+00:00   \n",
       "...          ...                  ...                        ...   \n",
       "8109        9332  1536673294782709760  2022-06-14 11:33:58+00:00   \n",
       "8110        9333  1536525461077262336  2022-06-14 01:46:31+00:00   \n",
       "8111        9335  1536515216665899013  2022-06-14 01:05:49+00:00   \n",
       "8112        9336  1536358141377384448  2022-06-13 14:41:39+00:00   \n",
       "8113        9337  1536251604772745216  2022-06-13 07:38:19+00:00   \n",
       "\n",
       "            username   latitude   longitude  \\\n",
       "0       MaseebAkhter  24.972077   67.064381   \n",
       "1       PolsekPlaju3  -3.190822  104.794609   \n",
       "2       irfania_real  31.315389   74.222395   \n",
       "3         kenny_thok  -2.455096  111.930698   \n",
       "4        kejarirohul   0.902542  100.308263   \n",
       "...              ...        ...         ...   \n",
       "8109   StellaSuwardi  -6.297602  106.770437   \n",
       "8110  natasya_puspa4  -6.364100  106.799599   \n",
       "8111    Agistasyahka  -6.301652  106.974561   \n",
       "8112      Ardi_Wdyto  -7.776581  113.198255   \n",
       "8113       gemaposID  -6.218042  106.857762   \n",
       "\n",
       "                                            final_tweet  \n",
       "0                                      rd jumma mubarak  \n",
       "1     lawan covid sinergitas tni polri cegah tular c...  \n",
       "2                                        jummah mubarak  \n",
       "3     semangat ya neng isolasi pakai sek pondok deri...  \n",
       "4     kamis tanggal april kepala seksi intelijen ari...  \n",
       "...                                                 ...  \n",
       "8109  vaksinasi booster tingkat imunitas varian omic...  \n",
       "8110  jokowi menteri waspada hati hati varian omicro...  \n",
       "8111  sebar virus omicron ba ba cepat banding varian...  \n",
       "8112                                     varian omicron  \n",
       "8113  breaking news omicron ba ba resmi deteksi indo...  \n",
       "\n",
       "[8114 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('HasilPreProcess.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99047a6e-ec55-4072-91d9-b251d18ba3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>final_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1387917769501515778</td>\n",
       "      <td>2021-04-29 23:52:37+00:00</td>\n",
       "      <td>MaseebAkhter</td>\n",
       "      <td>24.972077</td>\n",
       "      <td>67.064381</td>\n",
       "      <td>rd jumma mubarak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1387914934533513220</td>\n",
       "      <td>2021-04-29 23:41:21+00:00</td>\n",
       "      <td>PolsekPlaju3</td>\n",
       "      <td>-3.190822</td>\n",
       "      <td>104.794609</td>\n",
       "      <td>lawan covid sinergitas tni polri cegah tular c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1387913431865610244</td>\n",
       "      <td>2021-04-29 23:35:22+00:00</td>\n",
       "      <td>irfania_real</td>\n",
       "      <td>31.315389</td>\n",
       "      <td>74.222395</td>\n",
       "      <td>jummah mubarak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1387911958393987075</td>\n",
       "      <td>2021-04-29 23:29:31+00:00</td>\n",
       "      <td>kenny_thok</td>\n",
       "      <td>-2.455096</td>\n",
       "      <td>111.930698</td>\n",
       "      <td>semangat ya neng isolasi pakai sek pondok deri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1387907774441566213</td>\n",
       "      <td>2021-04-29 23:12:54+00:00</td>\n",
       "      <td>kejarirohul</td>\n",
       "      <td>0.902542</td>\n",
       "      <td>100.308263</td>\n",
       "      <td>kamis tanggal april kepala seksi intelijen ari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8109</th>\n",
       "      <td>1536673294782709760</td>\n",
       "      <td>2022-06-14 11:33:58+00:00</td>\n",
       "      <td>StellaSuwardi</td>\n",
       "      <td>-6.297602</td>\n",
       "      <td>106.770437</td>\n",
       "      <td>vaksinasi booster tingkat imunitas varian omic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8110</th>\n",
       "      <td>1536525461077262336</td>\n",
       "      <td>2022-06-14 01:46:31+00:00</td>\n",
       "      <td>natasya_puspa4</td>\n",
       "      <td>-6.364100</td>\n",
       "      <td>106.799599</td>\n",
       "      <td>jokowi menteri waspada hati hati varian omicro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8111</th>\n",
       "      <td>1536515216665899013</td>\n",
       "      <td>2022-06-14 01:05:49+00:00</td>\n",
       "      <td>Agistasyahka</td>\n",
       "      <td>-6.301652</td>\n",
       "      <td>106.974561</td>\n",
       "      <td>sebar virus omicron ba ba cepat banding varian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8112</th>\n",
       "      <td>1536358141377384448</td>\n",
       "      <td>2022-06-13 14:41:39+00:00</td>\n",
       "      <td>Ardi_Wdyto</td>\n",
       "      <td>-7.776581</td>\n",
       "      <td>113.198255</td>\n",
       "      <td>varian omicron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>1536251604772745216</td>\n",
       "      <td>2022-06-13 07:38:19+00:00</td>\n",
       "      <td>gemaposID</td>\n",
       "      <td>-6.218042</td>\n",
       "      <td>106.857762</td>\n",
       "      <td>breaking news omicron ba ba resmi deteksi indo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8114 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                       date        username  \\\n",
       "0     1387917769501515778  2021-04-29 23:52:37+00:00    MaseebAkhter   \n",
       "1     1387914934533513220  2021-04-29 23:41:21+00:00    PolsekPlaju3   \n",
       "2     1387913431865610244  2021-04-29 23:35:22+00:00    irfania_real   \n",
       "3     1387911958393987075  2021-04-29 23:29:31+00:00      kenny_thok   \n",
       "4     1387907774441566213  2021-04-29 23:12:54+00:00     kejarirohul   \n",
       "...                   ...                        ...             ...   \n",
       "8109  1536673294782709760  2022-06-14 11:33:58+00:00   StellaSuwardi   \n",
       "8110  1536525461077262336  2022-06-14 01:46:31+00:00  natasya_puspa4   \n",
       "8111  1536515216665899013  2022-06-14 01:05:49+00:00    Agistasyahka   \n",
       "8112  1536358141377384448  2022-06-13 14:41:39+00:00      Ardi_Wdyto   \n",
       "8113  1536251604772745216  2022-06-13 07:38:19+00:00       gemaposID   \n",
       "\n",
       "       latitude   longitude                                        final_tweet  \n",
       "0     24.972077   67.064381                                   rd jumma mubarak  \n",
       "1     -3.190822  104.794609  lawan covid sinergitas tni polri cegah tular c...  \n",
       "2     31.315389   74.222395                                     jummah mubarak  \n",
       "3     -2.455096  111.930698  semangat ya neng isolasi pakai sek pondok deri...  \n",
       "4      0.902542  100.308263  kamis tanggal april kepala seksi intelijen ari...  \n",
       "...         ...         ...                                                ...  \n",
       "8109  -6.297602  106.770437  vaksinasi booster tingkat imunitas varian omic...  \n",
       "8110  -6.364100  106.799599  jokowi menteri waspada hati hati varian omicro...  \n",
       "8111  -6.301652  106.974561  sebar virus omicron ba ba cepat banding varian...  \n",
       "8112  -7.776581  113.198255                                     varian omicron  \n",
       "8113  -6.218042  106.857762  breaking news omicron ba ba resmi deteksi indo...  \n",
       "\n",
       "[8114 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb67958d-37c8-4507-b0a7-eafe23f02173",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_tweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mlist\u001b[39m(X[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import pandas as pd from sklearn.feature_extraction.text \n",
    "import TfidfTransformer from sklearn.feature_extraction.text \n",
    "import CountVectorizer \n",
    "\n",
    "X = df['final_tweet']\n",
    "\n",
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24e88520-91a2-4c3d-887a-704e0c5ed89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from sklearn.feature_extraction.text import TfidfVectorizer\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(0,1))\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# tfidf = tfidf.fit_transform(X_train).toarray()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer \n\u001b[1;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_tweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m tfidfconverter \u001b[38;5;241m=\u001b[39m TfidfVectorizer(sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m))  \n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(0,1))\n",
    "# tfidf = tfidf.fit_transform(X_train).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# df = df.sample(frac=1, random_state=1)\n",
    "\n",
    "X = df['final_tweet']\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(sublinear_tf=True, min_df=100,encoding='utf-8', ngram_range=(0,1))  \n",
    "X = tfidfconverter.fit_transform(X.values.astype('U'))\n",
    "# .toarray()\n",
    "X\n",
    "\n",
    "df = pd.DataFrame(X.T.todense(), \n",
    "                  index=X.get_feature_names(), \n",
    "                  columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"], ascending=False)\n",
    "\n",
    "\n",
    "#cosine -> dokumen \n",
    "#split token kata ke 1 dokumen -> identify distribusi kata -> nentuin min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849a4fe0-6911-45e5-9022-8e7c8c059a4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a740127c-b089-4ce4-ac08-ffe3e8340268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b88fe3c-b8aa-4de5-b983-13615edd1baa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee4b122c-789a-4d43-ba64-5ae6aeb58075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.47318550121866937,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.24784774447953767,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.42739872183516847,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.41334508232564426,\n",
       " 0.0,\n",
       " 0.4362212688543733,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.41334508232564426,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a142d1cf-595a-4255-907b-1f1ec6e36dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72844a12-dd61-4609-981e-e4a127cf7a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create TF-IDF matrix\u001b[39;00m\n\u001b[0;32m      6\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 7\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_tweet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print feature names\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "File \u001b[1;32m~\\.conda\\envs\\skripsitwit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\skripsitwit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\skripsitwit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\.conda\\envs\\skripsitwit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32m~\\.conda\\envs\\skripsitwit\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:239\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_tweet = df['final_tweet']\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(final_tweet)\n",
    "\n",
    "# Print feature names\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# Print TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccb02e-8495-4921-bb2d-09ef2fd259f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
